% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/random_sample_imputation.R
\name{init_with_random_samples}
\alias{init_with_random_samples}
\title{Random Sample Imputation function}
\usage{
init_with_random_samples(
  sc,
  sdf,
  column = NULL,
  checkpointing = TRUE,
  checkpoint_frequency = 10
)
}
\arguments{
\item{sc}{A Spark connection}

\item{sdf}{A Spark DataFrame}

\item{column}{The column(s) to impute. If NULL, all columns will be imputed}

\item{checkpointing}{Default TRUE. Can be set to FALSE if you are running the package without access to a HDFS directory for checkpointing. It is strongly recommended to keep it to TRUE to avoid Stackoverflow errors.}

\item{checkpoint_frequency}{Advanced parameter, modify with care. If checkpointing = TRUE, how often to checkpoint , default = 10, so after processing every 10 variables, the lineage will be cut and the current state of computation will be save to disk. A low number might slow down computation but enable bigger computation. A number too high (or not checkpoiting) might cause JVM stackOverflowError as the lineage will have grown too big.}
}
\value{
The Spark DataFrame with missing values imputed
}
\description{
This function imputes missing values in a Spark DataFrame using random samples from the observed values.
}
\examples{
#TBD
}

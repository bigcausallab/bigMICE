% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mice.spark.R
\name{mice.spark.plus}
\alias{mice.spark.plus}
\title{MICE+ for Spark DataFrames using Sparklyr and Spark MLlib}
\usage{
mice.spark.plus(
  data,
  sc,
  variable_types,
  analysis_formula,
  where_missing,
  m = 5,
  method = NULL,
  predictorMatrix = NULL,
  formulas = NULL,
  modeltype = NULL,
  maxit = 5,
  printFlag = TRUE,
  seed = NA,
  imp_init = NULL,
  checkpointing = TRUE,
  checkpoint_frequency = 10,
  ...
)
}
\arguments{
\item{data}{A Spark DataFrame, the original data with extra missing values}

\item{sc}{A Spark connection}

\item{variable_types}{A named character vector, the variable types of the columns in the data.}

\item{analysis_formula}{A formula, the formula to use for the analysis}

\item{where_missing}{A logical vector, the locations of the missing values in the data}

\item{m}{The number of imputations to perform}

\item{method}{A character vector, the imputation method to use for each variable. If NULL, the function will infer the method based on the variable types.}

\item{predictorMatrix}{A matrix, the predictor matrix to use for the imputation. TBD}

\item{formulas}{A list, the formulas to use for the imputation. If NULL, the function will infer the formulas based on the other variables present in the data. TBD}

\item{modeltype}{A character vector, the model type to use for the imputation. If NULL, the function will infer the model type based on the variable types. The methods specified must match the order of the variables and must be one of "Logistic","Mult_Logistic","Linear","RandomForestClassifier","RandomForestRegressor" or "none".}

\item{maxit}{The maximum number of iterations to perform}

\item{printFlag}{A boolean, whether to print debug information}

\item{seed}{An integer, the seed to use for reproducibility}

\item{imp_init}{A Spark DataFrame, the original data with missing values, but with initial imputation (by random sampling or mean/median/mode imputation). Can be set to avoid re-running the initialisation step. Otherwise, the function will perform the initialisation step using the MeMoMe function.}

\item{checkpointing}{Default TRUE. Can be set to FALSE if you are running the package without access to a HDFS directory for checkpointing. It is strongly recommended to keep it to TRUE to avoid Stackoverflow errors.}

\item{checkpoint_frequency}{Advanced parameter, modify with care. If checkpointing = TRUE, how often to checkpoint , default = 10, so after processing every 10 variables, the lineage will be cut and the current state of computation will be save to disk. A low number might slow down computation but enable bigger computation. A number too high (or not checkpoiting) might cause JVM stackOverflowError as the lineage will have grown too big.}

\item{...}{Additional arguments to be passed to the function. TBD}
}
\value{
A list containing the Rubin's statistics for the model parameters, the per-imputation statistics, the imputation statistics, and the model parameters.
}
\description{
This function imputes missing values in a Spark DataFrame using MICE (Multiple Imputation by Chained Equations) algorithm. Additionally, it allows to look at the imputed values to see if they are reasonable and measure the uncertainty of the imputation.
}
\examples{
# Example for mice.spark.plus function
# Create complete data (without extra missing values)
#complete_data <- data.frame(
 # outcome = c(1, 0, 1, 1, 0, 0),
 # age = c(25, 30, 35, 28, 45, 32),
 # income = c(50000, 60000, 70000, 55000, 80000, 52000),
 # education = c("High", "Medium", "High", "Low", "Low", "Medium")
#)

# Copy complete data to Spark
#sdf_complete <- copy_to(sc, complete_data, "complete_data")

# Create where_missing indicator (logical vector)
#where_missing <- c(FALSE, TRUE, TRUE, FALSE, TRUE, TRUE)  # Indicates artificially missing

# Run MICE+
#mice_plus_result <- mice.spark.plus(
 # data = sdf,  # Data with missing values
 # data_true = sdf_complete,  # Complete data
 # sc = sc,
 # variable_types = variable_types,
 # analysis_formula = analysis_formula,
 # where_missing = where_missing,
 # m = 3,
 # maxit = 2,
 # printFlag = TRUE,
 # seed = 123,
 # checkpointing = FALSE
#)

# View results including known missings
#mice_plus_result$rubin_stats
#mice_plus_result$known_missings[[1]] \%>\% collect()  # First imputation

# Clean up
#spark_disconnect(sc)
}
